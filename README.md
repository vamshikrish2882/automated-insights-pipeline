
<p align="center">
  <h1>Automated Insights Pipeline (SQL + Python)</h1>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/SQL-Analytics-blue" />
  <img src="https://img.shields.io/badge/Python-Pandas-green" />
  <img src="https://img.shields.io/badge/ETL-Pipeline-orange" />
  <img src="https://img.shields.io/badge/Automation-Airflow Ready-yellow" />
</p>

This project showcases an **end-to-end automated insights pipeline** designed for a fictional operations and fulfillment dataset.  
It processes **50,000+ synthetic order and shipment records**, applies SQL transformations, computes weekly KPIs, detects anomalies, and generates visual reports that support real-world decision making.

The goal of this project is to demonstrate how data analysts and analytics engineers can build reliable, modular, and automation-ready analytics workflows using **Python, SQL, and Pandas**.

---

## ğŸš€ What This Pipeline Does

This pipeline replicates the type of analytical workflows commonly used across operations, supply chain, and fulfillment teams.

It automatically performs:

- **Data Generation** â€“ creates 50K+ rows of realistic operational data  
- **Extract & Load** â€“ ingests raw CSVs into a SQLite â€œanalytics storeâ€  
- **SQL Transformations** â€“ builds a clean analytical data model (`order_shipments`)  
- **KPI Calculations** â€“ computes weekly metrics like on-time rate and revenue  
- **Anomaly Detection** â€“ flags unusual performance and revenue fluctuations  
- **Automated Reporting** â€“ generates CSV outputs and time-series charts  
- **One-click Pipeline Execution** â€“ run everything using one command

---

## ğŸ§° Technology Stack

- **Python:** pandas, numpy, matplotlib  
- **SQL:** SQLite + SQL analytical modeling  
- **File Formats:** CSV  
- **Design Principles:** modular code, reproducible workflows, Airflow-ready structure  

---

## ğŸ—‚ Project Structure

```
automated-insights-pipeline/
â”‚
â”œâ”€â”€ data/                     
â”‚   â”œâ”€â”€ raw/                  # raw input data files
â”‚   â”œâ”€â”€ interim/              # optional staging files
â”‚   â””â”€â”€ processed/            # cleaned & final outputs
â”‚
â”œâ”€â”€ reports/                  
â”‚   â”œâ”€â”€ kpi_summary.csv       # generated weekly KPIs
â”‚   â”œâ”€â”€ anomalies.csv         # anomaly detection results
â”‚   â””â”€â”€ figures/              # time-series plots
â”‚       â”œâ”€â”€ weekly_total_revenue.png
â”‚       â””â”€â”€ weekly_on_time_rate.png
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ schema.sql            # database schema
â”‚   â””â”€â”€ transformations.sql   # analytical model creation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py             # path & configuration settings
â”‚   â”œâ”€â”€ db.py                 # SQLite helpers
â”‚   â”œâ”€â”€ generate_data.py      # synthetic data generator
â”‚   â”œâ”€â”€ extract_load.py       # CSV â†’ SQLite loader
â”‚   â”œâ”€â”€ transform.py          # runs SQL transformations
â”‚   â”œâ”€â”€ kpis.py               # computes weekly metrics
â”‚   â”œâ”€â”€ anomalies.py          # outlier/anomaly detector
â”‚   â”œâ”€â”€ report.py             # generates visual charts
â”‚   â””â”€â”€ run_pipeline.py       # orchestrates entire pipeline
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Pipeline Flow

### 1. **Data Generation**  
Creates synthetic operational datasets (orders + shipments).

### 2. **Extract & Load**  
Imports raw CSV files into a SQLite database defined by `schema.sql`.

### 3. **SQL Transformations**  
Creates the consolidated analytical table `order_shipments`, including:

- delivery lead time  
- days past required date  
- late shipment flag  
- weekly grouping (`order_week`)  

### 4. **KPI Calculation**  
Outputs weekly performance metrics such as:

- on-time shipment rate  
- total revenue  
- late shipment count  
- average delivery duration  
- average SLA delay  

### 5. **Anomaly Detection**  
Flags unusual weeks using simple z-score thresholds:

- âš ï¸ abnormal performance  
- ğŸ“‰ revenue drops  
- ğŸ“ˆ revenue spikes  

### 6. **Reporting & Visualization**  
Exports:

- `reports/kpi_summary.csv`  
- `reports/anomalies.csv`  
- Time-series performance charts  

---

## ğŸ“ˆ Sample Outputs

### ğŸ“„ KPI Summary
File location:
```
reports/kpi_summary.csv
```

Includes:
```
- On-time shipment rate
- Total weekly revenue
- Late shipment count
- Avg. delivery days
- Avg. days past required date
```

---

### ğŸš¨ Anomaly Report
File location:
```
reports/anomalies.csv
```

Flags:
```
- âš ï¸ Low on-time performance
- ğŸ“‰ Revenue declines
- ğŸ“ˆ Revenue spikes
```

---

### ğŸ–¼ Visual Reports
Saved in:
```
reports/figures/
```

Files:
```
- weekly_total_revenue.png
- weekly_on_time_rate.png
```

---

## â–¶ï¸ How to Run the Pipeline

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Run the entire pipeline
```bash
python -m src.run_pipeline
```

### 3. Generate visual reports
```bash
python -m src.report
```

All generated outputs appear in the **reports/** directory.

---

## ğŸ’¡ Why This Project Matters

This automated insights pipeline demonstrates key industry skills:

- End-to-end ETL & data modeling  
- SQL-based analytical transformations  
- KPI design for operations  
- Anomaly detection logic  
- Automated reporting with Python  
- Modular, production-ready project structure  

A strong portfolio project for **Data Analyst, Analytics Engineer, or Data Engineering** roles.

---

## ğŸ‘¤ About the Author

**Vamshikrishna Pandilla**  
Data Analyst â€¢ SQL â€¢ Python â€¢ Analytics Engineering  
ğŸ“§ Email: vamshikrishnapandilla908@gmail.com  
ğŸ”— LinkedIn: https://www.linkedin.com/in/vamshikrishna-pandilla
